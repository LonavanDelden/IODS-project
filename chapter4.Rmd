# 4. Clustering and classification

Read in the data and explore.

```{r data}
library(MASS)
data("Boston")
str(Boston)
summary(Boston)
```

- Draw a correlation matrix to look at relationships between variables

```{r cor}
library(corrplot)
library(tidyverse)
cor_matrix <- cor(Boston) %>% round(digits = 2)
corrplot(cor_matrix, method="circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)
```

- Establish the data frame

```{r scaling}
boston_scaled <- scale(Boston)
boston_scaled <- as.data.frame(boston_scaled)
summary(boston_scaled)
```

- Categorize the crime data into quantiles to scale low to high crime rates

```{r quantiles}
bins <- quantile(boston_scaled$crim)
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))
table(crime)
boston_scaled <- dplyr::select(boston_scaled, -crim)
boston_scaled <- data.frame(boston_scaled, crime)
```

- Split the data into train and test set to validate the model and adjust the data frame

```{r train}
n <- nrow(boston_scaled)
ind <- sample(n,  size = n * 0.8)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]
correct_classes <- test$crime
test <- dplyr::select(test, -crime)
str(train)
str(test)
```

- LDA analysis with crime as target variable, plot the data and create biplot arrows

```{r LDA}
lda.fit <- lda(crime ~ ., data = train)
lda.fit
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
classes <- as.numeric(train$crime)
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1.4)
```

- LDA is predicting classes with the test dataset and the outpot is compared with the real data in a cross table

```{r LDA prediction}
lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)
```

- Now we are moving on to clustering by working on the original dataset Boston. The data will be prepared and explored and clustered by K means. The color seperates to three clusters and only the 5 variables are shown to better read the matrix.

```{r clustering}
library(MASS)
data('Boston')
dist_eu <- dist(Boston)
dist_man <- dist(Boston, method = "manhattan")
summary(dist_eu)
summary(dist_man)
km <-kmeans(Boston, centers = 3)
pairs(Boston[6:10], col = km$cluster)
```

- K-means: determine the k and visualize

Bonus: Perform k-means on the original Boston data with some reasonable number of clusters (> 2). Remember to standardize the dataset. Then perform LDA using the clusters as target classes. Include all the variables in the Boston data in the LDA model. Visualize the results with a biplot (include arrows representing the relationships of the original variables to the LDA solution). Interpret the results. Which variables are the most influencial linear separators for the clusters?

```{r Kmean}
library(MASS)
library(ggplot2)
data('Boston')
boston_scaled <- scale(Boston)
boston_scaled <- as.data.frame(boston_scaled)
set.seed(123)
k_max <- 10
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})
qplot(x = 1:k_max, y = twcss, geom = 'line')
km <-kmeans(boston_scaled, centers = 2)
pairs(Boston, col = km$cluster)
```

Super-Bonus: 

boston_scaled <- scale(Boston)
boston_scaled <- as.data.frame(boston_scaled)
summary(boston_scaled)

model_predictors <- dplyr::select(train, -crime)
# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)

install.packages(plotly)
library(plotly)

3Dplot:
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers')

Adjust the code: add argument color as a argument in the plot_ly() function. Set the color to be the crime classes of the train set. Draw another 3D plot where the color is defined by the clusters of the k-means. How do the plots differ? Are there any similarities?

before kniting:
library(GGally)
library(ggplot2)
