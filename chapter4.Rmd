
# 4. Clustering and classification

Read in the data and explore. The dataset is on Housing Values in Boston Suburbs in the USA. The variables include crime statistics and information on residency types and values, such as access to infrastructure, schooling etc. Please find the summary below.

```{r data, echo=TRUE, results='markup', message=FALSE, warning=FALSE}
library(MASS)
data("Boston")
str(Boston)
summary(Boston)
```

- Graphical overview of the data and a correlation matrix to look at relationships between variables:

The graphical overview shows that the variables are not normally distributed (maybe except for "rm" - rooms per dwelling).

The matrix is highlighting the strongest positive correlation between the accessibility to radial highways ("rad") and the full-value property-tax rate ("tax), which means that the property taxes increase substantially when it is close to long distance connections, which is crucial for many industries.

The strongest negative correlations are between the weighted mean of distances to five Boston employment centres ("dis") with age of the building, nitrogen oxide concentrations ("nox") and the proportion of non-retail business acres per town ("indus"). This means when the distance to the employment centres increases, the buildung age, nox levels and non-retail businesses decreases, which makes sense as new buildings are more common in the outskirt of town (easier to build new than to renovate), less polution from traffic and the businesses like the crowded center better as well. Another strong negative correlation is between the lower status of the population ("lstat") and the median value of owner-occupied homes ("medv"), which is a no-brainer.

```{r cor, echo=TRUE, results='markup', message=FALSE, warning=FALSE}
library(corrplot)
library(tidyverse)
library(GGally)
ggpairs(Boston, upper = list(continuous = wrap("cor", size=2.6)))
cor_matrix <- cor(Boston) %>% round(digits = 2)
corrplot(cor_matrix, method="circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)
```

- Establish the data frame

The data set is standardized by scaling the variables according to their mean and standard deviation. Please see the summary below. The mean is now 0 for all the variables and the values are overall a lot lower as they are now scaled to their vairiable dependent values instead of real values.

```{r scaling, echo=TRUE, results='markup', message=FALSE, warning=FALSE}
boston_scaled <- scale(Boston)
boston_scaled <- as.data.frame(boston_scaled)
summary(boston_scaled)
```

- Categorize the crime data into quantiles to scale low to high crime rates

```{r quantiles, echo=TRUE, results='markup', message=FALSE, warning=FALSE}
bins <- quantile(boston_scaled$crim)
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))
table(crime)
boston_scaled <- dplyr::select(boston_scaled, -crim)
boston_scaled <- data.frame(boston_scaled, crime)
```

- Split the data into train and test set to validate the model and adjust the data frame

```{r train, echo=TRUE, results='markup', message=FALSE, warning=FALSE}
n <- nrow(boston_scaled)
ind <- sample(n,  size = n * 0.8)
train <- boston_scaled[ind,]
train_cluster <- boston_scaled[ind,]
test <- boston_scaled[-ind,]
correct_classes <- test$crime
test <- dplyr::select(test, -crime)
str(train)
str(test)
```

- LDA analysis with crime as target variable, plot the data and create biplot arrows

```{r LDA, echo=TRUE, results='markup', message=FALSE, warning=FALSE}
lda.fit <- lda(crime ~ ., data = train)
lda.fit
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
classes <- as.numeric(train$crime)
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1.4)
```

- LDA is predicting classes with the test dataset and the output is compared with the real data in a cross table

The LDA model results in about a third of the classes being predicted wrong when compared to the real data (70 classes correct as in low=low etc. and 32 classes wrong as in low=low_med etc).

```{r LDA prediction, echo=TRUE, results='markup', message=FALSE, warning=FALSE}
lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)
```

- Clustering with K-means

Working on the original dataset "Boston"", which is scaled again. The data are prepared, explored and clustered by K means. The k plot determined two clusters with the strong change below 2.5. Therefore the clusters for the visualization was set to two. The matrix visualizes all variables in when plotted against each other, therefore the top and the bottom are mirrored.

```{r clustering, echo=TRUE, results='markup', message=FALSE, warning=FALSE}
library(MASS)
data('Boston')
boston_scaled1 <- scale(Boston)
boston_scaled1 <- as.data.frame(boston_scaled1)
dist_eu <- dist(boston_scaled1)
dist_man <- dist(boston_scaled1, method = "manhattan")
summary(dist_eu)
summary(dist_man)
set.seed(123)
k_max <- 10
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled1, k)$tot.withinss})
qplot(x = 1:k_max, y = twcss, geom = 'line')
km <-kmeans(boston_scaled1, centers = 2)
pairs(boston_scaled1, col = km$cluster)
```

To see the variables more clearly, the plot was cut into smaller pieces of 5 variables max. Not all variables show a clear clustering when compared to each other but e.g. proportion of non-retail businesses (indus), nitrogen oxides (nox),building age (age) and median home value (medv) seperate quite well into clusters when compared to other variables. Especially nitrogen oxides (nox) show clear cluster seperation around the mean when paired with age, rm and dis etc. Just to clarify, the data are scaled, which means in this case the cluster seperate the data in relation to the mean. 

```{r cluster2, echo=TRUE, results='markup', message=FALSE, warning=FALSE}
library(MASS)
km <-kmeans(boston_scaled1, centers = 2)
pairs(boston_scaled1[1:5], col = km$cluster)
pairs(boston_scaled1[5:10], col = km$cluster)
pairs(boston_scaled1[10:14], col = km$cluster)
```

- Bonus

K-means performed on the original Boston data and standarzied by scaling with 3 clusters. The proportion of residential land zoned for lots over 25,000 sq.ft.s (zn) is the most influencial variable.

```{r Kmean, echo=TRUE, results='markup', message=FALSE, warning=FALSE}
library(MASS)
library(ggplot2)
data('Boston')
boston_scaled2 <- scale(Boston)
km <-kmeans(boston_scaled2, centers = 3)
cluster <- km$cluster
boston_scaled2 <- data.frame(boston_scaled2, cluster)
lda.fit2 <- lda(cluster ~ ., data = boston_scaled2)
lda.fit2
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
classes2 <- as.numeric(boston_scaled2$cluster)
plot(lda.fit2, dimen = 2, col = classes2, pch = classes2, main = "LDA biplot using three clusters 1, 2 and 3")
lda.arrows(lda.fit2, myscale = 1.4)
```

- Super-Bonus: 

```{r superbonus, echo=TRUE, results='markup', message=FALSE, warning=FALSE}
boston_scaled <- scale(Boston)
boston_scaled <- as.data.frame(boston_scaled)
summary(boston_scaled)
lda.fit <- lda(crime ~., data = train)
model_predictors <- dplyr::select(train, -crime)
dim(train_cluster)
dim(model_predictors)
dim(lda.fit$scaling)
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling # if %*% is not working, check the dimensions! Second of first line needs to match first of the second line
matrix_product <- as.data.frame(matrix_product)
library(plotly)
p1 <- plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers')
p1
p2 <- plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = train_cluster$crime)
p2
train2 <- dplyr::select(train, -crime)
km3 <-kmeans(train2, centers = 3)
p3 <- plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = km3$cluster)
p3
```



before kniting:
library(GGally)
library(ggplot2)
